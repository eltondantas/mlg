---
title: "Modelando Preços de Venda de Imóveis com Modelo Linear Normal"
author: "Elton Dantas de Oliveira Mesquita"
output:
  html_document:
    theme: readable
    highlight: breezedark
    toc: true
    toc_depth: 5
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{=html}
<style> body {text-align: justify} </style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(rmarkdown); pandoc_version()
```

## Pacotes

```{r}
library(tidyverse)
library(plotly)
library(GGally)
library(gridExtra)
library(ggfortify)
library(hnp)
library(nortest)
```

## Dados

Exemplo disponível em: Paula, G. A. (2013). [Modelos de Regressão com Apoio Computacional](https://www.ime.usp.br/~giapaula/texto_2013.pdf). São Paulo, SP: IME-USP. (Exercício 23, página 112). Dados disponíveis em: <https://www.ime.usp.br/~giapaula/textoregressao.htm>

Neste trabalho, vamos modelar o preço de venda de imóveis a partir de dados relativos a uma amostra de 27 imóveis. As variáveis desse conjunto de dados são:

-   ***imposto***: imposto do imóvel (em US\$ 100);

-   ***areaT***: área do terreno (em 1.000 pés quadrados);

-   ***areaC***: área construída (em 1.000 pés quadrados);

-   ***idade***: idade da residência (em anos);

-   ***preco***: preço de venda do imóvel (em US\$ 1.000).

Sendo assim, o objetivo dessa modelo é explicar a variável ***preço de venda*** a partir das variáveis ***área do terreno***, ***área construída*** e ***idade***.

```{r}
dados = read.table("imoveis.dat")
colnames(dados) = c("imposto", "areaT", "areaC", "idade", "preco")
attach(dados)
dados

```

<br>

## Análise Descritiva

### Box-plots

O gráfico interativo a baixo nos mostra algumas medidas resumo para cada uma das variáveis.

```{r}
plot_ly(type = "box") %>%
  add_trace(y = imposto, name = "imposto") %>% 
  add_trace(y = areaT, name = "área do terreno") %>% 
  add_trace(y = areaC, name = "área construída") %>%
  add_trace(y = idade, name = "idade do imóvel") %>%
  add_trace(y = preco, name = "preço de venda")
```

### Densidades, Dispersões e Correlações

```{r}
g = ggpairs(dados, aes(color = I("slategray"), fill = I("slategray")),
            lower = list(continuous = wrap("smooth",col="black")),
            diag=list(continuous = wrap("densityDiag",alpha=0.5,size=1)))
ggplotly(g)
```

Nos gráficos descritivos acima, percebemos que todas as variáveis possuem distribuições assimétricas e que a variável preço possui altas correlações positivas com as demais, exceto com a variável idade, em que a correlação é fraca e negativa. Em outras palavras, quanto maiores os valores de imposto, área do terreno e área construída, maior o valor do preço de venda. Também identificamos a presença de outliers, mais evidentes nos gráficos de box-plot.

<br>

## Ajuste 1

Nesse primeiro ajuste, consideraremos para o nosso modelo todas as variáveis disponíveis e o chamaremos de modelo completo. Partiremos do modelo normal linear.

$$
preço = \beta_0 + \beta_1*imposto + \beta_2*areaC + \beta_3*areaT + \beta_4*idade + \epsilon
$$

### Modelo Completo

```{r}
model1 = lm(preco~.,dados)
summary(model1)
```

Verificamos que, para o modelo completo, as variáveis imposto e área construída são significativas e obtivemos um $R^2$ muito alto, indicando um possível bom ajuste aos dados amostrais. Porém, o $R^2$ por si só não nos garante que o modelo seja o mais adequado.

A seguir, faremos a análise de diagnóstico para verificar a adequação do modelo.

<br>

## Diagnóstico 1

### Normalidade e Envelope

Traçaremos agora os gráficos de histograma e envelope, para avaliar o ajuste do modelo normal.

```{r}
hist(model1$residuals, probability = TRUE, main = 'Resíduos do Ajuste',
      xlab = 'Resíduos', ylab = 'Frequência', col = "gray",)
lines(density(model1$residuals), col = "slategray", lwd = 4)

hnp(model1, main = 'Gráfico Normal com Envelope',
    xlab = 'Quantis Teóricos',
    ylab = 'Resíduos',
    pch = 16,halfnormal =  FALSE, resid.type = "standard")
```

<br>

### Testes de Normalidade dos Resíduos

Para os testes a seguir, consideramos

$H_0$: Os resíduos têm distribuição normal.

```{r}
# Kolmogorov - Smirnov
ks.test(model1$residuals,"pnorm")
# Lilliefors
lillie.test(model1$residuals)
# Cramer-von-Mises
cvm.test(model1$residuals)
# Shapiro-Wilk
shapiro.test(model1$residuals)
# Shepiro-Francia
sf.test(model1$residuals)
# Anderson-Darling
ad.test(model1$residuals)
```

Ao nível de 5% de significância, não rejeitamos a hipótese de que os resíduos tenham distribuição normal.

<br>

### Análise de Resíduos

```{r}
plot(lm(model1, dados), which = 1:6, pch = 16,lwd = 3,
     caption = c("Resíduos x Valores ajustados",
                 "Normal Q-Q",
                 "Resíduos padronizados x Valores ajustados",
                 "Distância de Cook",
                 "Resíduos x Alavancagem"))
```

No gráfico de Resíduos x Valores Ajustados, verifica-se uma mudança na faixa de variação dos pontos, isto é, indícios de heterocedasticidade. Há também outliers

<br>

### Medidas de Influência

-   DFBetas (***dfb***): estatísticas que indicam o efeito da remoção de cada observação sobre as estimativas dos parâmetros do modelo (apontam os pontos influentes);

-   DFFit (***dffit***) e Cook's D (***cook.d***): são estatísticas que indicam o efeito da remoção de cada observação sobre os valores preditos/ajustados do modelo (apontam os pontos aberrantes);

-   COVRATIO (***cov.r***): estatística que indica o efeito da remoção de cada observação sobre a matriz de covariâncias do modelo, em outras palavras, mede a alteração na precisão das estimativas dos parâmetros do modelo (aponta os pontos alavanca);

-   HAT (***hat***): diagonal da matriz de projeção ($H = X(X'X)^{-1}X'$) da solução dos mínimos quadrados.

```{r}
inf = influence.measures(model1) 
summary(inf)

# A soma da diagonal da matriz H é o número de parâmetros do modelo
# sum(inf$infmat[,"hat"])
```

A partir da tabela acima, temos que:

-   Observação 8: ponto influente;

-   Observação 9: ponto de alavanca;

-   Observação 10: ponto aberrante;

-   Observação 27: ponto influente, aberrante e de alavanca.

<br>

## Ajuste 2

A seguir, tentaremos melhorar o ajuste anterior, selecionando variáveis através da técnica AIC. Mas, além de todas as variáveis do modelo completo, consideraremos também as suas interações.

<br>

### Seleção de Variáveis (AIC)

```{r}
model = lm(preco~(.)^2,dados)
MASS::stepAIC(model)
```

Inicialmente, verificamos que imposto e área construída eram as únicas variáveis significativas para o modelo e elas também foram selecionadas pela técnica AIC. Considerando as interações das variáveis do modelo completo, a seleção por AIC nos sugere também a variável imposto:areaT, que relaciona o imposto à área do terreno. Chamaremos esse ajuste de modelo reduzido e repetiremos as técnicas de diagnóstico realizadas no modelo completo.

<br>

### Modelo Reduzido

```{r}
model2 = lm(preco~imposto+areaT+areaC+imposto:areaT,dados)
summary(model2)
```

## Diagnóstico 2

### Normalidade e Envelope

```{r}
hist(model2$residuals, probability = TRUE, main = 'Resíduos do Ajuste',
      xlab = 'Resíduos', ylab = 'Frequência', col = "gray",)
lines(density(model$residuals), col = "slategray", lwd = 4)

hnp(model2, main = 'Gráfico Normal com Envelope',
    xlab = 'Quantis Teóricos',
    ylab = 'Resíduos',
    pch = 16)
```

### Testes de Normalidade dos Resíduos

```{r}
# Kolmogorov - Smirnov
ks.test(model2$residuals,"pnorm")
# Lilliefors
lillie.test(model2$residuals)
# Cramer-von-Mises
cvm.test(model2$residuals)
# Shapiro-Wilk
shapiro.test(model2$residuals)
# Shepiro-Francia
sf.test(model2$residuals)
# Anderson-Darling
ad.test(model2$residuals)
```

<br>

### Análise de Resíduos

```{r}
plot(lm(model2, dados), which = 1:4, pch = 16,lwd = 3,
     caption = c("Resíduos x Valores ajustados",
                 "Normal Q-Q",
                 "Resíduos padronizados x Valores ajustados",
                 "Distância de Cook",
                 "Resíduos x Alavancagem"))
```

<br>

### Medidas de Influência

-   DFBetas (***dfb***): estatísticas que indicam o efeito da remoção de cada observação sobre as estimativas dos parâmetros do modelo (apontam os pontos influentes);

-   DFFit (***dffit***) e Cook's D (***cook.d***): são estatísticas que indicam o efeito da remoção de cada observação sobre os valores preditos/ajustados do modelo (apontam os pontos aberrantes);

-   COVRATIO (***cov.r***): estatística que indica o efeito da remoção de cada observação sobre a matriz de covariâncias do modelo, em outras palavras, mede a alteração na precisão das estimativas dos parâmetros do modelo (aponta os pontos alavanca);

-   HAT (***hat***): diagonal da matriz de projeção ($H = X(X'X)^{-1}X'$) da solução dos mínimos quadrados.

```{r}
inf = influence.measures(model2) 
summary(inf)

# A soma da diagonal da matriz H é o número de parâmetros do modelo
# sum(inf$infmat[,"hat"])
```

A partir da tabela acima, temos que:

-   Observação 9: ponto aberrante;

-   Observação 10: ponto de alavanca;

-   Observação 27: ponto influente, aberrante e de alavanca.

<br>

## Ajuste 3

A seguir, tentaremos melhorar o modelo reduzido anterior, agora aplicando a função log à variável resposta preço e selecionando, novamente, as variáveis explicativas através da técnica AIC. Chamaremos esse ajuste de modelo 3.

### Modelo 3

```{r}
model = lm(log(preco)~imposto+areaC+areaT+imposto:areaT,dados)
summary(model)

MASS::stepAIC(model)
```

Pelo critério AIC, nosso modelo 3ficará com as variáveis imposto, área construída e área do terreno.

```{r}
model3 = lm(log(preco)~imposto+areaC+areaT,dados)
summary(model)
```

<br>

## Diagnóstico 3

### Normalidade e Envelope

```{r}
hist(model3$residuals, probability = TRUE, main = 'Resíduos do Ajuste',
      xlab = 'Resíduos', ylab = 'Frequência', col = "gray",)
lines(density(model3$residuals), col = "slategray", lwd = 4)

hnp(model3, main = 'Gráfico Normal com Envelope',
    xlab = 'Quantis Teóricos',
    ylab = 'Resíduos',
    pch = 16)
```

<br>

### Análise de Resíduos

```{r}
plot(lm(model3, dados), which = 1:4, pch = 16,lwd = 3,
     caption = c("Resíduos x Valores ajustados",
                 "Normal Q-Q",
                 "Resíduos padronizados x Valores ajustados",
                 "Distância de Cook",
                 "Resíduos x Alavancagem"))
```

<br>

### Medidas de Influência

-   DFBetas (***dfb***): estatísticas que indicam o efeito da remoção de cada observação sobre as estimativas dos parâmetros do modelo (apontam os pontos influentes);

-   DFFit (***dffit***) e Cook's D (***cook.d***): são estatísticas que indicam o efeito da remoção de cada observação sobre os valores preditos/ajustados do modelo (apontam os pontos aberrantes);

-   COVRATIO (***cov.r***): estatística que indica o efeito da remoção de cada observação sobre a matriz de covariâncias do modelo, em outras palavras, mede a alteração na precisão das estimativas dos parâmetros do modelo (aponta os pontos alavanca);

-   HAT (***hat***): diagonal da matriz de projeção ($H = X(X'X)^{-1}X'$) da solução dos mínimos quadrados.

```{r}
inf = influence.measures(model3) 
summary(inf)

# A soma da diagonal da matriz H é o número de parâmetros do modelo
# sum(inf$infmat[,"hat"])
```

A partir da tabela acima, temos que:

-   Observação 8; ponto influente, aberrante e de alavanca;

-   Observação 9: ponto de alavanca;

-   Observação 10: ponto de alavanca;

-   Observação 27: ponto influente, aberrante e de alavanca.

## Interpretação

```{r}
model3$coefficients
```

$$
preço = 2.83 + 0.05*imposto + 0.18*areaC + 0.015*areaT
$$

Por fim, com o modelo 3, temos que o preço de venda sobe, em média, 0.05 a cada unidade aumentada na variável imposto, 0.18 na variável área construída e 0.015 na variável área do terreno.
