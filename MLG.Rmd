---
title: "Aplicações de Modelos Lineares Generalizados"
author: "Elton Dantas de Oliveira Mesquita"
output:
  html_document:
    theme: readable
    highlight: breezedark
    toc: true
    toc_depth: 6
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{=html}
<style> body {text-align: justify} </style>
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(rmarkdown); pandoc_version()
```

## Pacotes

```{r}
library(tidyverse)
library(psych)
library(plotly)
library(GGally)
library(hnp)
library(car)
library(nortest)
```

# Modelo Linear Normal

Para as aplicações a seguir, consideraremos o modelo de regressão normal linear

$$
y_i = \beta_1 + \beta_2x_{2i} + ... + \beta_px_{pi} + \epsilon_i, \quad i = 1,...,n
$$

em que os erros $e_i$ são variáveis aleatórias independentes, normalmente distribuídas, de média zero e variância $\sigma^2$ constante.

<br>

## Previsão de Vendas de Imóveis

### Dados

Exemplo disponível em: Paula, G. A. (2013). [Modelos de Regressão com Apoio Computacional](https://www.ime.usp.br/~giapaula/texto_2013.pdf). São Paulo, SP: IME-USP. (Exercício 23, página 112). Dados disponíveis em: <https://www.ime.usp.br/~giapaula/textoregressao.htm>

Neste exemplo, vamos modelar o preço de venda de imóveis a partir de dados relativos a uma amostra de 27 imóveis. As variáveis do conjunto de dados são:

-   ***imposto***: imposto do imóvel (em US\$ 100);

-   ***areaT***: área do terreno (em 1.000 pés quadrados);

-   ***areaC***: área construída (em 1.000 pés quadrados);

-   ***idade***: idade da residência (em anos);

-   ***preco***: preço de venda do imóvel (em US\$ 1.000).

Sendo assim, o nosso objetivo é encontrar o melhor ajuste linear, nesse caso, que explique e quantifique a variável ***preço de venda***, a partir das demais variáveis.

$$
preço_i = \beta_0 + \beta_1imposto_i + \beta_2areaC_i + \beta_3areaT_i + \beta_4idade_i
$$

```{r}
# Obtendo os dados
dados = read.table("dados/imoveis.dat")
colnames(dados) = c("imposto", "areaT", "areaC", "idade", "preco")
attach(dados)

# Algumas observações dos dados
head(dados)

# Medidas descritivas
describe(dados)
```

<br>

### Análise Descritiva

#### Box-plots

```{r}
plot_ly(,type = "box") %>%
  add_trace(y = imposto, name = "imposto") %>% 
  add_trace(y = areaT, name = "área do terreno") %>% 
  add_trace(y = areaC, name = "área construída") %>%
  add_trace(y = idade, name = "idade do imóvel") %>%
  add_trace(y = preco, name = "preço de venda")
```

Em um resumo descritivo dos dados, observamos altas variâncias das variáveis idade e preço, destacando-se das demais. Com uma idade mediana de 40 anos, temos um imóvel de apenas 3 anos de idade. E com um preço mediano de \$US\$\$36,900, temos dois imóveis bem mais caros, custando \$US\$ 82,900 e \$US\$ 84,900.

Os gráficos de box-plot nos mostram que os dados possuem outliers e que as variáveis possuem distribuições assimétricas. Focando na nossa variável alvo, preço, os pontos destoantes são justamente os dois imóveis mais caros.

<br>

#### Densidades, Dispersões e Correlações

```{r}
g = ggpairs(dados, aes(color = I("slategray"), fill = I("slategray")),
            lower = list(continuous = wrap("smooth",col="black")),
            diag=list(continuous = wrap("densityDiag",alpha=0.5,size=1)))
ggplotly(g)
```

No gráfico acima, as curvas representadas na diagonal principal deixam mais evidente a constatação de assimetria observada nos box-plots. A variável preço possui correlações positivas fortes com as variáveis imposto, área do terreno e área construída, mas uma correlação negativa fraca com a variável idade. Em outras palavras, quanto maiores os valores de imposto, área construída, área do terreno, maior o preço de venda do imóvel.

Podemos observar também que há correlações relevantes entre as variáveis explicativas imposto, área construída e área do terreno. Isto nos dá indícios de multicolinearidade e, assim, uma possível redundância de informação. Então, talvez um modelo completo com todas as variáveis não seja o melhor.

<br>

### Ajuste 1

Nesse primeiro ajuste, consideraremos para o nosso modelo todas as variáveis disponíveis e o chamaremos de modelo completo.

$$
preço = \beta_0 + \beta_1*imposto + \beta_2*areaC + \beta_3*areaT + \beta_4*idade
$$

#### Modelo Completo

```{r}
modelc = lm(preco~.,dados)
summary(modelc)
```

Verificamos que, para o modelo completo, as variáveis imposto e área construída são as únicas significativas e obtivemos um $R^2$ de 0.92, muito alto, indicando um possível bom ajuste aos dados amostrais. Porém, o $R^2$ por si só não nos garante que o modelo seja o mais adequado.

A seguir, faremos a análise de diagnóstico para verificar a adequação do modelo.

<br>

#### Diagnóstico

```{r}
qqPlot(modelc, pch = 16, main = "Envelope")

# Para comparar com uma distribuição específica
# qqPlot(model1$residuals, distribution = "gamma", shape=10)
```

Teste de Normalidade

$H_0$: Os resíduos têm distribuição normal.

```{r}
# Shapiro-Wilk
shapiro.test(modelc$residuals)
```

Ao nível de 5% de significância, não rejeitamos a hipótese de que os resíduos tenham distribuição normal. No gráfico de envelope para o ajuste normal, os pontos estão dentro das bandas, porém a presença de ondulações é indício de variância não constante. Vamos verificar se de fato isso ocorre no gráfico de resíduos por valores ajustados.

```{r}
residualPlot(modelc,pch=16,main="Resíduos x Valores ajustados")
```

Embora não haja violação do pressuposto de normalidade dos resíduos, o gráfico de Resíduos x Valores ajustados deixa evidente a heterocedasticidade, isto é, a variância não constante. Portanto, o ajuste com todas as variáveis não é o mais adequado.

<br>

### Ajuste 2

#### Seleção de variáveis

Aplicaremos o critério de informação de Akaike (AIC) ao modelo completo, para selecionar as variáveis que deverão permanecer.

```{r}
MASS::stepAIC(modelc)
```

O AIC nos retornou as variáveis imposto e área construída como aquelas que deverão ser mantidas no modelo, o qual chamaremos de modelo parcimonioso.

<br>

#### Modelo Parcimonioso

```{r}
modelp = lm(preco~imposto+areaC,dados)
summary(modelp)
```

Novamente, as variáveis imposto e área construída são bastante significativas.

<br>

#### Diagnóstico

```{r}
qqPlot(modelp, pch = 16, main = "Envelope")
```

```{r}
shapiro.test(modelp$residuals)
```

A 5% de significância não rejeitamos a hipótese de que os resíduos tenham distribuição normal. O gráfico de envelope também não aponta indícios que a normalidade esteja sendo violada, porém as ondulações indicam variância não constante.

```{r}
residualPlot(modelp, pch = 16, main = "Resíduos x Valores ajustados")
```

Novamente, temos um modelo que atende o pressuposto de normalidade dos resíduos, mas viola o de homocedasticidade.

<br>

### Ajuste 3

Tentaremos corrigir a falta de ajuste do modelo parcimonioso, aplicando a função logaritmíca na variável preço.

#### Modelo parcimonioso com logartimo

```{r}
mlog = lm(log(preco)~imposto+areaC,dados)
summary(mlog)
```

Obtemos, então, um modelo em que as variáveis imposto e área construída continuam bastante significativas e, ainda, o intercepto também.

#### Diagnóstico

```{r}
qqPlot(mlog, pch = 16, main = "Envelope")
```

```{r}
shapiro.test(mlog$residuals)
```

```{r}
residualPlot(mlog, type = "rstudent",
             pch = 16,main = "Resíduos studentizados x Valores ajustados")
```

Pelo gráfico de envelope, não há indícios de que o pressuposto de normalidade tenha sido violado. No gráfico de Resíduos x Valor ajustado, temos um cenário melhor que os anteriores, com uma variância mais próxima da homogeneidade. Sem considerar os dois pontos mais afastados, observamos uma nuvem de pontos mais dispersos na faixa de -2 a 2 que nos gráficos anteriores.

<br>

#### Pontos Influentes

Partiremos agora para a investigação de possíveis pontos influentes, que podem estar atrapalhando a qualidade do ajuste.

Medidas de influência:

-   DFBetas (***dfb***): estatísticas que indicam o efeito da remoção de cada observação sobre as estimativas dos parâmetros do modelo;

-   DFFit (***dffit***) e Cook's D (***cook.d***): são estatísticas que indicam o efeito da remoção de cada observação sobre os valores ajustados do modelo;

-   COVRATIO (***cov.r***): estatística que indica o efeito da remoção de cada observação sobre a matriz de covariâncias do modelo, em outras palavras, mede a alteração na precisão das estimativas dos parâmetros do modelo;

-   HAT (***hat***): diagonal da matriz de projeção ($H = X(X'X)^{-1}X'$) da solução dos mínimos quadrados, é a métrica de alavancagem.

```{r}
inf = influence.measures(mlog)
summary(inf)
```

Baseando-se nas medidas de influência acima, que verificam o efeito da remoção de cada uma das observações , temos que os possíveis pontos de influência são as observações 9, 10 e 27. Percebemos também que a observação 27 é a mais crítica, por impactar nas estimativas dos parâmetros, nos valores ajustados e na variância do modelo. Também podemos constatar isso graficamente.

```{r}
plot(lm(mlog, dados), which = 4, pch = 16,lwd = 3,
     caption = c("","","","Distância de Cook x Observação"))
abline(h=1,col="red",lty=2)

# 2ª opção
# plot(inf$infmat[,"cook.d"], pch = 16,
#      xlab = "observação", ylab = "distância de cook", main = "Distância de Cook")
# abline(h=1,col="red",lty=2)
# text(inf$infmat[,"cook.d"],pos = 3,offset=0.2)
```

```{r}
cut = 2*3/27
plot(inf$infmat[,"hat"], pch = 16,
     xlab = "observação", ylab = "medida h", main = "Alavancagem")
abline(h = cut, col = "red",lty = 2)
text(inf$infmat[,"hat"], pos = 3, offset = 0.2)
```

As observações 9, 10 e 27 são referentes aos dados:

```{r}
dados[c(9,10,27),c("imposto","areaC")]
```

```{r}
mlog.sem9
```

```{r}
mlog.sem10
```

```{r}
mlog.sem27
```
